{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Text Recognition using CRNN and CTC Loss\n",
    "\n",
    "**Author:** [Joshua Newton](https://github.com/joshuacwnewton) and [Parthiv Chigurupati](https://github.com/parthivc), based on the work of [Mike Henry](https://github.com/mbhenry/)<br>\n",
    "**Date created:** 2020/06/18<br>\n",
    "**Last modified:** 2020/06/18<br>\n",
    "**Description:** End-to-end text recognition using a recurrent model trained on synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Temporarily disable GPU for building on local machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example shows how to train a neural network to recognize text in images.  We use Keras's functional API to implement a model based on the CRNN architecture. We also create both a custom loss function to adapt the CTC loss provided by `tensorflow.keras.backend`, as well as a custom metric to adapt `tf.edit_distance`.\n",
    "\n",
    "To demonstrate this setup, we build a training dataset by generating synthetic images from a source list of words. To implement the dataset, we subclass `tensorflow.keras.utils.Sequence`. It provides an `on_epoch_end` callback, which we use for curriculum learning by gradually ramping up the difficulty of the dataset during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "One external package is required for this example.\n",
    "* `cairocffi` provides Python bindings for `cairo`, a 2D vector graphics library written in the C programming language. We use this package to generate synthetic images of text.\n",
    "\n",
    "If you've opened this example in Google Colab, you can install the package like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install cairocffi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then, import the required Tensorflow dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# TensorFlow packages\n",
    "from tensorflow.keras import layers, losses, models, optimizers, utils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing a synthetic dataset\n",
    "\n",
    "We train our neural network on a dataset made of synthetic images. The synthetic images are generated on the fly from a list of source text sequences. The images appear as black printed text on a white background, with image processing to add realistic noise.\n",
    "\n",
    "> _**Note:** Other text-image datasets can be found in existing OCR research literature. These datasets are often designed for more challenging OCR applications, such as scene text recognition with non-horizontal text. If you plan to adapt this tutorial for your own work, consider using one of the following datasets: [MJSynth (MJ)](http://www.robots.ox.ac.uk/~vgg/data/text/), [SynthText (ST)](http://www.robots.ox.ac.uk/~vgg/data/scenetext/), [IIIT](http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html), [SVT](http://www.iapr-tc11.org/mediawiki/index.php/The_Street_View_Text_Dataset), [IC03](http://www.iapr-tc11.org/mediawiki/index.php/ICDAR_2003_Robust_Reading_Competitions), [IC13](http://rrc.cvc.uab.es/?ch=2), [IC15](http://rrc.cvc.uab.es/?ch=4), [SVTP](http://openaccess.thecvf.com/content_iccv_2013/papers/Phan_Recognizing_Text_with_2013_ICCV_paper.pdf), and/or [CUTE](http://cs-chan.com/downloads_CUTE80_dataset.html)._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Downloading the source word lists\n",
    "\n",
    "First, let's download and uncompress an archive containing two text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tgz_path = Path(\n",
    "    utils.get_file(\n",
    "        \"wordlists.tgz\",\n",
    "        origin=\"http://www.mythic-ai.com/datasets/wordlists.tgz\",\n",
    "        untar=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_dir = tgz_path.parent\n",
    "monogram_file = dataset_dir / \"wordlist_mono_clean.txt\"\n",
    "bigram_file = dataset_dir / \"wordlist_bi_clean.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Each file contains a different set of [**n-gram**](https://en.wikipedia.org/wiki/N-gram) data samples:\n",
    "* `wordlist_mono_clean.txt` contains a list of **monograms**, or single words. These words have been pre-sorted according to their frequency in English speech.\n",
    "* `wordlist_bi_clean.txt` contains a list of **bigrams**, or pairs of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Loading and preprocessing words\n",
    "\n",
    "Before using these word lists to create a dataset, we filter out any unsuitable examples as we load them into memory.\n",
    "\n",
    "We use 2 criteria for filtering:\n",
    "\n",
    "* `ABSOLUTE_MAX_STRING_LEN` restricts the length of allowable text sequences.\n",
    "* `ALPHABET` contains every allowable character. To simplify the example, we attempt to recognize only lowercase alphabetic characters.\n",
    "\n",
    "We then combine the monograms and bigrams into a single source list of text seqeunces, then sort them based on length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ABSOLUTE_MAX_STRING_LEN = 16\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "\n",
    "\n",
    "def preprocess_words(wordlist_file, valid_characters, max_string_len=None):\n",
    "    def _is_valid_str(in_str):\n",
    "        search = re.compile(rf\"^[{valid_characters} ]+$\", re.UNICODE).search\n",
    "        return bool(search(in_str))\n",
    "\n",
    "    def _is_length_of_word_valid(word):\n",
    "        return (\n",
    "            max_string_len == -1\n",
    "            or max_string_len is None\n",
    "            or len(word) <= max_string_len\n",
    "        )\n",
    "\n",
    "    suitable_words = []\n",
    "    with wordlist_file.open() as f:\n",
    "        for line in f:\n",
    "            word = line.rstrip()\n",
    "            if _is_valid_str(word) and _is_length_of_word_valid(word):\n",
    "                suitable_words.append(word)\n",
    "\n",
    "    return suitable_words\n",
    "\n",
    "\n",
    "monograms = preprocess_words(monogram_file, ALPHABET, ABSOLUTE_MAX_STRING_LEN)\n",
    "bigrams = preprocess_words(bigram_file, ALPHABET, ABSOLUTE_MAX_STRING_LEN)\n",
    "sequences = sorted(monograms + bigrams, key=len)\n",
    "\n",
    "print(f\"Wordlists contain {len(sequences)} suitable text sequences.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Set aside words for validation\n",
    "\n",
    "Here, we use a slightly more involved approach (compared to simple Python list indexing) to ensure that words of varying lengths are distributed evenly between the training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "\n",
    "def trn_val_split(input_list, split_ratio):\n",
    "    n = round(1 / split_ratio)\n",
    "    trn_list, val_list = [], []\n",
    "    for i, item in enumerate(input_list):\n",
    "        if i % n == 0:\n",
    "            val_list.append(item)\n",
    "        else:\n",
    "            trn_list.append(item)\n",
    "\n",
    "    return trn_list, val_list\n",
    "\n",
    "\n",
    "trn_sequences, val_sequences = trn_val_split(sequences, VAL_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Defining functions for creating synthetic text images\n",
    "\n",
    "We use several functions to generate images from strings of text. The functions include image transformations to introduce variations in the generated images. The image transformations are controlled using flags, which can be set during training to progressively increase the difficulty of the dataset. This is referred to as [curriculum learning](https://arxiv.org/abs/1904.03626).\n",
    "\n",
    "The image transformations include:\n",
    "* `multi_fonts`: Start by using only the `Courier` font to generate images. Then, later in training, expand the font list and choose a font at random from the list.\n",
    "* `ud`: Start by using a fixed y-coordinate for the text within the image. Then, later in training, add a random vertical shift to the y-coordinate.\n",
    "* `rotate`: Start by keeping text entirely horizontal. Then, later in training, add a slight random rotation to the text.\n",
    "* `speckle`: Create large \"blotches\" of noise, which look more realistic than simply adding gaussian noise.\n",
    "\n",
    "These aren't the only possible transformations, however. If you're interested in examples of other image transformations used to better synthesize real-world images, the [MJSynth](https://www.robots.ox.ac.uk/~vgg/data/text/) synthetic dataset would be a good place to start.\n",
    "These aren't the only possible transformations, however. If you're interested in examples of other image transformations used to better synthesize real-world images, the [MJSynth](https://www.robots.ox.ac.uk/~vgg/data/text/) synthetic dataset would be a good place to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import cairocffi as cairo\n",
    "\n",
    "\n",
    "def speckle(img):\n",
    "    severity = np.random.uniform(0, 0.6)\n",
    "    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n",
    "    img_speck = img + blur\n",
    "    img_speck[img_speck > 1] = 1\n",
    "    img_speck[img_speck <= 0] = 0\n",
    "    return img_speck\n",
    "\n",
    "\n",
    "def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n",
    "    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n",
    "    with cairo.Context(surface) as context:\n",
    "        # Create blank white canvas\n",
    "        context.set_source_rgb(1, 1, 1)\n",
    "        context.paint()\n",
    "\n",
    "        # Select the font\n",
    "        if multi_fonts:\n",
    "            fonts = [\n",
    "                \"Century Schoolbook\",\n",
    "                \"Courier\",\n",
    "                \"STIX\",\n",
    "                \"URW Chancery L\",\n",
    "                \"FreeMono\",\n",
    "            ]\n",
    "            context.select_font_face(\n",
    "                np.random.choice(fonts),\n",
    "                cairo.FONT_SLANT_NORMAL,\n",
    "                np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]),\n",
    "            )\n",
    "        else:\n",
    "            context.select_font_face(\n",
    "                \"Courier\", cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD\n",
    "            )\n",
    "        context.set_font_size(25)\n",
    "\n",
    "        # Determine coordinates for text within the image\n",
    "        box = context.text_extents(text)\n",
    "        border_w_h = (4, 4)\n",
    "        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n",
    "            raise IOError(\n",
    "                (\n",
    "                    \"Could not fit string into image. Max char count is \"\n",
    "                    \"too large for given image width.\"\n",
    "                )\n",
    "            )\n",
    "        max_shift_x = w - box[2] - border_w_h[0]\n",
    "        max_shift_y = h - box[3] - border_w_h[1]\n",
    "        top_left_x = np.random.randint(0, int(max_shift_x))\n",
    "        if ud:\n",
    "            top_left_y = np.random.randint(0, int(max_shift_y))\n",
    "        else:\n",
    "            top_left_y = h // 2\n",
    "\n",
    "        # Use coordinates to place text in image\n",
    "        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n",
    "        context.set_source_rgb(0, 0, 0)\n",
    "        context.show_text(text)\n",
    "\n",
    "    # Get image from cairo surface\n",
    "    buf = surface.get_data()\n",
    "    a = np.frombuffer(buf, np.uint8)\n",
    "    a.shape = (h, w, 4)\n",
    "    a = a[:, :, 0]\n",
    "    a = a.astype(np.float32) / 255\n",
    "    a = np.expand_dims(a, 0)\n",
    "\n",
    "    if rotate:\n",
    "        a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n",
    "\n",
    "    a = speckle(a)\n",
    "\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building the dataset using the `Sequence` class\n",
    "\n",
    "To contain our data, we subclass `tf.keras.utils.Sequence` rather than using Numpy arrays or generators. `Sequence` provides a handy `on_epoch_end` callback, which allows us to progressively increase the difficulty of the dataset during training.\n",
    "\n",
    "Within our class, we also define a `_build_word_subset` method which creates a subset of words to draw from. By operating on only a subset of the words, we can easily modify characteristics of the dataset during training, such as the dataset's size, the number of blank images the dataset contains, and the max word length for images.\n",
    "\n",
    "> _**Note:** If the max word length is increased, the generated text may exceed the input image dimensions. Before creating the model and dataset, make sure to set `IMG_W` to a size that accommodates for all of the word lengths you plan to train on. In this example, for a `MAX_WORD_LEN` of 4, `IMG_W` is set to `32 * MAX_WORD_LEN`, or 128. This is done to keep the training duration small, so that the workflow can be quickly demonstrated._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class TextImageDataset(utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_sequences,\n",
    "        batch_size,\n",
    "        img_w,\n",
    "        img_h,\n",
    "        downsample_factor,\n",
    "        subset_num=-1,\n",
    "        max_word_len=4,\n",
    "        blank_ratio=0,\n",
    "        start_epoch=1,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = start_epoch\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "        # Config settings for synthetic image generation function\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.rotate = False\n",
    "        self.ud = False\n",
    "        self.multi_fonts = False\n",
    "\n",
    "        # Base wordlist\n",
    "        random.shuffle(text_sequences)\n",
    "        self.text_sequences = text_sequences\n",
    "\n",
    "        # Config settings for creating a wordlist subset\n",
    "        self.subset_num = subset_num\n",
    "        self.max_word_len = max_word_len\n",
    "        self.blank_ratio = blank_ratio\n",
    "        self.text_subset = self._build_word_subset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.text_subset) // self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Width and height are backwards from typical Keras convention,\n",
    "        # because width is the time dimension when it gets fed into the RNN\n",
    "        X_batch = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n",
    "        y_batch = np.ones([self.batch_size, ABSOLUTE_MAX_STRING_LEN + 2])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text = self.text_subset[index + i]\n",
    "            X_batch[i, 0 : self.img_w, :, 0] = paint_text(\n",
    "                text, self.img_w, self.img_h, self.rotate, self.ud, self.multi_fonts\n",
    "            )[0, :, :].T\n",
    "            y_batch[i] = self._text_to_labels(text)\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def _text_to_labels(self, text):\n",
    "        # to use k.backend.ctc_batch_loss, 3 pieces of information are needed:\n",
    "        #   1. y_true (text, in integer form, padded out to max string length)\n",
    "        #   2. sequence length for each batch item in y_true\n",
    "        #   3. sequence length for each batch item in y_pred\n",
    "        if text == \"\":\n",
    "            int_text = [len(ALPHABET)]  # Additional int val for blank token\n",
    "        else:\n",
    "            int_text = [ALPHABET.find(c) for c in text]\n",
    "        y_true = int_text + [-1] * (ABSOLUTE_MAX_STRING_LEN - len(int_text))\n",
    "        y_true_len = [len(int_text)]\n",
    "        y_pred_len = [self.img_w // self.downsample_factor - 2]\n",
    "\n",
    "        # Keras losses only accept 2 inputs (label, y_pred), so we concatenate\n",
    "        label = y_pred_len + y_true_len + y_true\n",
    "\n",
    "        return label\n",
    "\n",
    "    def _build_word_subset(self):\n",
    "        string_list = []\n",
    "\n",
    "        for text in self.text_sequences:\n",
    "            if len(string_list) == self.subset_num:\n",
    "                break\n",
    "            if len(text) <= self.max_word_len:\n",
    "                string_list.append(text)\n",
    "\n",
    "        # \"This seems to be important for achieving translational invariance\"\n",
    "        for _ in range(round(len(string_list) * self.blank_ratio)):\n",
    "            string_list.append(\"\")\n",
    "\n",
    "        random.shuffle(string_list)\n",
    "\n",
    "        return string_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Update paint function parameters to implement curriculum learning\n",
    "        self.epoch_num += 1  # Sets for the next epoch (1-based index)\n",
    "        if self.epoch_num >= 3:\n",
    "            self.ud = True\n",
    "        if self.epoch_num >= 5:\n",
    "            self.multi_fonts = True\n",
    "        if self.epoch_num >= 7:\n",
    "            self.rotate = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we can create our training and validation datasets using the source word lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "SUBSET_NUM = 8000  # Number of words to pull from source lists into a subset\n",
    "MAX_WORD_LEN = 4  # Limit word length within subset\n",
    "BLANK_RATIO = 0.2  # Add (0.2 * SUBSET_NUM) blanks to the subset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_H = 64\n",
    "IMG_W = 32 * MAX_WORD_LEN  # Ensure text sequences can fit in generated images\n",
    "DOWNSAMPLE_FACTOR = 4\n",
    "\n",
    "trn_dataset = TextImageDataset(\n",
    "    trn_sequences,\n",
    "    BATCH_SIZE,\n",
    "    IMG_W,\n",
    "    IMG_H,\n",
    "    DOWNSAMPLE_FACTOR,\n",
    "    SUBSET_NUM,\n",
    "    MAX_WORD_LEN,\n",
    "    BLANK_RATIO,\n",
    ")\n",
    "val_dataset = TextImageDataset(\n",
    "    val_sequences, BATCH_SIZE, IMG_W, IMG_H, DOWNSAMPLE_FACTOR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, we can visualize images from a sample batch as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_batch = next(iter(trn_dataset))\n",
    "\n",
    "f, axarr = plt.subplots(5, 2)\n",
    "for i, ax in enumerate(f.axes):\n",
    "    # Image is in (W, H, 1) format. Squeeze changes this to (W, H), and .T\n",
    "    # transposes it to (H, W), allowing it to be displayed as grayscale image\n",
    "    ax.imshow(np.squeeze(sample_batch[0][i]).T, cmap=\"gray\", vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing the model, optimizer, loss function, and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Model setup\n",
    "\n",
    "For our model, we use a simplified CRNN with 2 bidirectional GRU layers. Notably, LSTMs can also be used in place of GRUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_filters = 16\n",
    "kernel_size = (3, 3)\n",
    "pool_size = 2\n",
    "time_dense_size = 32\n",
    "rnn_size = 512\n",
    "act = \"relu\"\n",
    "conv_to_rnn_dims = (\n",
    "    IMG_W // (pool_size ** 2),\n",
    "    (IMG_H // (pool_size ** 2)) * conv_filters,\n",
    ")\n",
    "\n",
    "# Input layer\n",
    "X = layers.Input(shape=(IMG_W, IMG_H, 1), dtype=\"float32\")\n",
    "\n",
    "# Convolution layers\n",
    "inner = layers.Conv2D(\n",
    "    conv_filters,\n",
    "    kernel_size,\n",
    "    padding=\"same\",\n",
    "    activation=act,\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    name=\"conv1\",\n",
    ")(X)\n",
    "inner = layers.MaxPooling2D(pool_size=(pool_size, pool_size), name=\"max1\")(inner)\n",
    "inner = layers.Conv2D(\n",
    "    conv_filters,\n",
    "    kernel_size,\n",
    "    padding=\"same\",\n",
    "    activation=act,\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    name=\"conv2\",\n",
    ")(inner)\n",
    "inner = layers.MaxPooling2D(pool_size=(pool_size, pool_size), name=\"max2\")(inner)\n",
    "inner = layers.Reshape(target_shape=conv_to_rnn_dims, name=\"reshape\")(inner)\n",
    "inner = layers.Dense(time_dense_size, activation=act, name=\"dense1\")(inner)\n",
    "\n",
    "# Recurrent layers: Two layers of bidirectional GRUs\n",
    "gru_1 = layers.Bidirectional(\n",
    "    layers.GRU(rnn_size, return_sequences=True, kernel_initializer=\"he_normal\"),\n",
    "    merge_mode=\"sum\",\n",
    "    name=\"bi_gru1\",\n",
    ")(inner)\n",
    "gru_2 = layers.Bidirectional(\n",
    "    layers.GRU(rnn_size, return_sequences=True, kernel_initializer=\"he_normal\"),\n",
    "    merge_mode=\"concat\",\n",
    "    name=\"bi_gru2\",\n",
    ")(gru_1)\n",
    "\n",
    "# transforms RNN output to character activations:\n",
    "inner = layers.Dense(len(ALPHABET) + 1, kernel_initializer=\"he_normal\", name=\"dense2\")(\n",
    "    gru_2\n",
    ")\n",
    "y_pred = layers.Activation(\"softmax\", name=\"softmax\")(inner)\n",
    "\n",
    "model = models.Model(inputs=X, outputs=y_pred)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(\n",
    "    learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "To train our model, we make use of [CTC Loss](https://en.wikipedia.org/wiki/Connectionist_temporal_classification), or connectionist temporal classification. Keras's backend provides an implementation of the CTC Loss function that we can make use of. This backend implementation requires 4 inputs: 3 variables for the true label (`y_true`, `input_length`, and `label_length`) and 1 variable for `y_pred`. This poses an issue, because Keras loss functions must have the function signature `func(y, y_pred)` to be used in `model.compile()`. We can get around this, however, by concatenating the 3 label variables into a single Tensor within the dataset, then decomposing the label inside a custom loss function.\n",
    "\n",
    "Also within this loss function, we make one additional modification to `y_pred` to remove parts of the softmax output that aren't useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ctc_loss(y, y_pred):\n",
    "    # Decompose label into its subsequent parts\n",
    "    input_length = y[:, 0:1]\n",
    "    label_length = y[:, 1:2]\n",
    "    y_true = y[:, 2:]\n",
    "\n",
    "    # From old example: \"the first couple outputs of the RNN tend to be garbage\"\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "A common way to compare the similarity between two strings is the [edit distance](https://en.wikipedia.org/wiki/Edit_distance) (often referred to as the Levenshtein distance). As well, Tensorflow provides an `edit_distance` function that we can use. Notably, however, this function requires that batches of input strings be in `SparseTensor` representation. So, we can't use the function as a Keras metric as-is.\n",
    "\n",
    "Thankfully, there are a number of backend functions in Keras and Tensorflow that help us prepare our input. These functions are used internally by Keras to compute the CTC loss, but we can also repurpose them here.\n",
    "\n",
    "* `K.ctc_label_dense_to_sparse` converts our `y_true` input batch into `SparseTensor` form.\n",
    "* `tf.nn.ctc_greedy_decoder` decodes the softmax output from the CRNN into a text string with the `SparseTensor` form. Note that this decoder is a special case of `tf.nn.ctc_beam_search_decoder`, which could be used for more complex (but time-consuming) decoding.\n",
    "\n",
    "We combine these functions together into a custom Keras metric function. Like the custom CTC Loss we created before, custom metrics require the specific function signature `func(y, y_true)`. So, like before, we work with a concatenated label containing all 3 pieces of label information, which we then split into its subsequent parts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "def edit_distance(y, y_pred):\n",
    "    # Decompose label into its subsequent parts\n",
    "    input_length = y[:, 0:1]\n",
    "    label_length = y[:, 1:2]\n",
    "    y_true = y[:, 2:]\n",
    "\n",
    "    # This form is required by ctc_label_dense_to_sparse and ctc_greedy_decoder\n",
    "    input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)\n",
    "    label_length = tf.cast(tf.squeeze(label_length, axis=-1), tf.int32)\n",
    "\n",
    "    # Get int64 SparseTensor representations of y_true and y_pred\n",
    "    y_true_sparse = K.ctc_label_dense_to_sparse(y_true, label_length)\n",
    "    y_true_sparse = tf.cast(y_true_sparse, tf.int64)\n",
    "\n",
    "    y_pred = tf.transpose(y_pred, [1, 0, 2])\n",
    "    decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(y_pred, input_length)\n",
    "    y_pred_sparse = decoded[0]\n",
    "\n",
    "    # Calculate Levenshtein distance for batch\n",
    "    dist_batch = tf.edit_distance(y_pred_sparse, y_true_sparse)\n",
    "\n",
    "    return tf.reduce_mean(dist_batch, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Bringing it all together: the training routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=ctc_loss, optimizer=sgd, metrics=[edit_distance])\n",
    "model.fit(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    shuffle=True,\n",
    "    validation_data=val_dataset,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_ocr",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}