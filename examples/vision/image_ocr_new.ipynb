{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ABSOLUTE_MAX_STRING_LEN = 16\n",
    "MINIBATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "ALPHABET = u'abcdefghijklmnopqrstuvwxyz '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and uncompress archive of raw word source lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fdir = os.path.dirname(\n",
    "    utils.get_file('wordlists.tgz',\n",
    "                   origin='http://www.mythic-ai.com/datasets/wordlists.tgz',\n",
    "                   untar=True)\n",
    ")\n",
    "monogram_file = os.path.join(fdir, 'wordlist_mono_clean.txt')\n",
    "bigram_file = os.path.join(fdir, 'wordlist_bi_clean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_word_list`: Function that builds a list of words satisfying the following criteria:\n",
    "* Only words with lowercase alphabetic characters and spaces are included\n",
    "* Words greater than `max_string_len` are excluded\n",
    "* The ratio of monograms to bigrams is made to equal to `mono_fraction`\n",
    "* Common words are interlaced with uncommon words (based on their frequency in English speech)\n",
    "* Mixing in blank words. Prevously handled by data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_list(num_words, max_string_len=None, mono_fraction=0.5):\n",
    "    assert max_string_len <= ABSOLUTE_MAX_STRING_LEN\n",
    "    assert num_words % MINIBATCH_SIZE == 0\n",
    "    assert (VAL_SPLIT * num_words) % MINIBATCH_SIZE == 0\n",
    "\n",
    "    string_list = [''] * num_words\n",
    "    tmp_string_list = []\n",
    "    X_text = []\n",
    "    Y_data = np.ones([num_words, ABSOLUTE_MAX_STRING_LEN]) * -1\n",
    "    Y_len = [0] * num_words\n",
    "    \n",
    "    def _text_to_labels(text):\n",
    "        ret = []\n",
    "        for char in text:\n",
    "            ret.append(ALPHABET.find(char))\n",
    "        return ret\n",
    "    \n",
    "    def _is_valid_str(in_str):\n",
    "        search = re.compile(r'^[a-z ]+$', re.UNICODE).search\n",
    "        return bool(search(in_str))\n",
    "\n",
    "    def _is_length_of_word_valid(word):\n",
    "        return (max_string_len == -1 or\n",
    "                max_string_len is None or\n",
    "                len(word) <= max_string_len)\n",
    "\n",
    "    # monogram file contains words sorted by frequency in english speech\n",
    "    with open(monogram_file, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if len(tmp_string_list) == int(num_words * mono_fraction):\n",
    "                break\n",
    "            word = line.rstrip()\n",
    "            if _is_length_of_word_valid(word):\n",
    "                tmp_string_list.append(word)\n",
    "\n",
    "    # bigram file contains common word pairings in english speech\n",
    "    with open(bigram_file, mode='r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if len(tmp_string_list) == num_words:\n",
    "                break\n",
    "            columns = line.lower().split()\n",
    "            word = columns[0] + ' ' + columns[1]\n",
    "            if _is_valid_str(word) and _is_length_of_word_valid(word):\n",
    "                tmp_string_list.append(word)\n",
    "                \n",
    "    if len(tmp_string_list) != num_words:\n",
    "        raise IOError('Could not pull enough words'\n",
    "                      'from supplied monogram and bigram files.')\n",
    "        \n",
    "    # interlace to mix up the easy and hard words\n",
    "    string_list[::2] = tmp_string_list[:num_words // 2]\n",
    "    string_list[1::2] = tmp_string_list[num_words // 2:]\n",
    "    \n",
    "    # insert blank words every 4th word\n",
    "    for i in range(0, num_words, 4):\n",
    "        string_list.insert(i, '')\n",
    "    string_list = string_list[:num_words]\n",
    "\n",
    "    for i, word in enumerate(string_list):\n",
    "        Y_len[i] = len(word)\n",
    "        Y_data[i, 0:len(word)] = _text_to_labels(word)\n",
    "        X_text.append(word)\n",
    "    Y_len = np.expand_dims(np.array(Y_len), 1)\n",
    "   \n",
    "    return X_text, Y_data, Y_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build initial wordlist of 16000 short monograms (len < 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_t, Y_d, Y_l = build_word_list(num_words=16000, max_string_len=4, mono_fraction=1)\n",
    "\n",
    "print(len(X_t))\n",
    "print(\"First five words:\")\n",
    "print(X_t[:5])\n",
    "print(\"\\n\" + \"First five words converted to integer labels:\")\n",
    "print(Y_d[:5])\n",
    "print(\"\\n\" + \"Length of each word:\")\n",
    "print(Y_l[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Callback class -> Sequence class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from scipy import ndimage\n",
    "import cairocffi as cairo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_H = 64\n",
    "IMG_W = 128\n",
    "DOWNSAMPLE_FACTOR = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for generating synthetic images from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this creates larger \"blotches\" of noise which look\n",
    "# more realistic than just adding gaussian noise\n",
    "# assumes greyscale with pixels ranging from 0 to 1\n",
    "\n",
    "def speckle(img):\n",
    "    severity = np.random.uniform(0, 0.6)\n",
    "    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n",
    "    img_speck = (img + blur)\n",
    "    img_speck[img_speck > 1] = 1\n",
    "    img_speck[img_speck <= 0] = 0\n",
    "    return img_speck\n",
    "\n",
    "\n",
    "# paints the string in a random location the bounding box\n",
    "# also uses a random font, a slight random rotation,\n",
    "# and a random amount of speckle noise\n",
    "\n",
    "def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n",
    "    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n",
    "    with cairo.Context(surface) as context:\n",
    "        context.set_source_rgb(1, 1, 1)  # White\n",
    "        context.paint()\n",
    "        # this font list works in CentOS 7\n",
    "        if multi_fonts:\n",
    "            fonts = [\n",
    "                'Century Schoolbook', 'Courier', 'STIX',\n",
    "                'URW Chancery L', 'FreeMono']\n",
    "            context.select_font_face(\n",
    "                np.random.choice(fonts),\n",
    "                cairo.FONT_SLANT_NORMAL,\n",
    "                np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n",
    "        else:\n",
    "            context.select_font_face('Courier',\n",
    "                                     cairo.FONT_SLANT_NORMAL,\n",
    "                                     cairo.FONT_WEIGHT_BOLD)\n",
    "        context.set_font_size(25)\n",
    "        box = context.text_extents(text)\n",
    "        border_w_h = (4, 4)\n",
    "        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n",
    "            raise IOError(('Could not fit string into image.'\n",
    "                           'Max char count is too large for given image width.'))\n",
    "\n",
    "        # teach the RNN translational invariance by\n",
    "        # fitting text box randomly on canvas, with some room to rotate\n",
    "        max_shift_x = w - box[2] - border_w_h[0]\n",
    "        max_shift_y = h - box[3] - border_w_h[1]\n",
    "        top_left_x = np.random.randint(0, int(max_shift_x))\n",
    "        if ud:\n",
    "            top_left_y = np.random.randint(0, int(max_shift_y))\n",
    "        else:\n",
    "            top_left_y = h // 2\n",
    "        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n",
    "        context.set_source_rgb(0, 0, 0)\n",
    "        context.show_text(text)\n",
    "\n",
    "    buf = surface.get_data()\n",
    "    a = np.frombuffer(buf, np.uint8)\n",
    "    a.shape = (h, w, 4)\n",
    "    a = a[:, :, 0]  # grab single channel\n",
    "    a = a.astype(np.float32) / 255\n",
    "    a = np.expand_dims(a, 0)\n",
    "    if rotate:\n",
    "        a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n",
    "    a = speckle(a)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextImageSequence(utils.Sequence):\n",
    "    def __init__(self, X_text, Y_data, Y_len, batch_size,\n",
    "                 img_w, img_h, downsample_factor, start_epoch=0):\n",
    "        self.X_text = X_text\n",
    "        self.Y_data = Y_data\n",
    "        self.Y_len = Y_len\n",
    "        self.batch_size = batch_size\n",
    "        self.img_w = img_w \n",
    "        self.img_h = img_h \n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.epoch_num = start_epoch\n",
    "        self.rotate=False\n",
    "        self.ud=False\n",
    "        self.multi_fonts=False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_text)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # width and height are backwards from typical Keras convention\n",
    "        # because width is the time dimension when it gets fed into the RNN\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            X_data = np.ones([self.batch_size, 1, self.img_w, self.img_h])\n",
    "        else:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n",
    "\n",
    "        labels = np.ones([self.batch_size, ABSOLUTE_MAX_STRING_LEN])\n",
    "        input_length = np.zeros([self.batch_size, 1])\n",
    "        label_length = np.zeros([self.batch_size, 1])\n",
    "        source_str = []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X_data[i, 0, 0:self.img_w, :] = (\n",
    "                    paint_text(self.X_text[index + i],\n",
    "                               self.img_w, self.img_h,\n",
    "                               self.rotate,\n",
    "                               self.ud,\n",
    "                               self.multi_fonts)[0, :, :].T\n",
    "                )\n",
    "            else:\n",
    "                X_data[i, 0:self.img_w, :, 0] = (\n",
    "                    paint_text(self.X_text[index + i],\n",
    "                               self.img_w, self.img_h,\n",
    "                               self.rotate,\n",
    "                               self.ud,\n",
    "                               self.multi_fonts)[0, :, :].T\n",
    "                )\n",
    "            labels[i, :] = self.Y_data[index + i]\n",
    "            input_length[i] = self.img_w // self.downsample_factor - 2\n",
    "            label_length[i] = self.Y_len[index + i]\n",
    "            source_str.append(self.X_text[index + i])\n",
    "            \n",
    "        inputs = {'the_input': X_data,\n",
    "                  'the_labels': labels,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                  'source_str': np.array(source_str)  # used for visualization only\n",
    "                  }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}  # dummy data for dummy loss function\n",
    "        return inputs, outputs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # update paint function parameters to implement curriculum learning\n",
    "        self.epoch_num += 1\n",
    "        if self.epoch_num >= 2:\n",
    "            self.ud=True\n",
    "        if self.epoch_num >= 5:\n",
    "            self.multi_fonts=True\n",
    "        if self.epoch_num >= 8:\n",
    "            self.rotate=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sequence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequence = TextImageSequence(X_t, Y_d, Y_l, BATCH_SIZE, \n",
    "                             IMG_W, IMG_H, DOWNSAMPLE_FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize images from a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_batch = next(iter(sequence))\n",
    "\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "for i, ax in enumerate(f.axes):\n",
    "    # Image is in (W, H, 1) format. Squeeze changes this to (W, H), and .T\n",
    "    # transposes it to (H, W), allowing it to be displayed as grayscale image\n",
    "    ax.imshow(np.squeeze(sample_batch[0][\"the_input\"][i]).T,\n",
    "              cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of changes during migration from TextImageGenerator to TextImageSequence:\n",
    "* `on_epoch_begin` -> `on_epoch_end` (-1 to each epoch value)\n",
    "* Move blank word insertion to `build_word_list` (so blank words will be used for validation, too)\n",
    "* Leave out second wordlist (32000, with 12-len word) from on_epoch_end in favor of making a new sequence and a new model.fit() call.\n",
    "* Remove lambda function usage for wrapping paint_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}